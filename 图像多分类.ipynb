{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tarfile\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((150, 150)),  #becasue vgg takes 150*150\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "\n",
    "])\n",
    "\n",
    "#Augmentation is not done for test/validation data.\n",
    "transform_test = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((150, 150)),  #becasue vgg takes 150*150\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(\"dataset/train/\", transform=transform_train)\n",
    "test_ds = ImageFolder(\"dataset/val/\", transform=transform_test)\n",
    "pred_ds = ImageFolder(\"dataset/pred/\", transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6341, 1450, 1450)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds), len(pred_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dl = DataLoader(test_ds, batch_size, num_workers=4, pin_memory=True)\n",
    "pred_dl = DataLoader(pred_ds, batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absa',\n",
       " 'adidas',\n",
       " 'adobe',\n",
       " 'airbnb',\n",
       " 'alibaba',\n",
       " 'aliexpress',\n",
       " 'allegro',\n",
       " 'amazon',\n",
       " 'ameli_fr',\n",
       " 'american_express',\n",
       " 'anadolubank',\n",
       " 'aol',\n",
       " 'apple',\n",
       " 'arnet_tech',\n",
       " 'aruba',\n",
       " 'att',\n",
       " 'azul',\n",
       " 'bahia',\n",
       " 'banco_de_occidente',\n",
       " 'banco_inter',\n",
       " 'bankia',\n",
       " 'barclaycard',\n",
       " 'barclays',\n",
       " 'bbt',\n",
       " 'bcp',\n",
       " 'bestchange',\n",
       " 'blizzard',\n",
       " 'bmo',\n",
       " 'bnp_paribas',\n",
       " 'bnz',\n",
       " 'boa',\n",
       " 'bradesco',\n",
       " 'bt',\n",
       " 'caixa_bank',\n",
       " 'canada',\n",
       " 'capital_one',\n",
       " 'capitec',\n",
       " 'cathay_bank',\n",
       " 'cetelem',\n",
       " 'chase',\n",
       " 'cibc',\n",
       " 'cloudconvert',\n",
       " 'cloudns',\n",
       " 'cogeco',\n",
       " 'commonwealth_bank',\n",
       " 'cox',\n",
       " 'crate_and_barrel',\n",
       " 'cryptobridge',\n",
       " 'daum',\n",
       " 'db',\n",
       " 'dhl',\n",
       " 'dkb',\n",
       " 'docmagic',\n",
       " 'dropbox',\n",
       " 'ebay',\n",
       " 'eharmony',\n",
       " 'erste',\n",
       " 'etisalat',\n",
       " 'etrade',\n",
       " 'facebook',\n",
       " 'fibank',\n",
       " 'file_transfer',\n",
       " 'fnac',\n",
       " 'fsnb',\n",
       " 'godaddy',\n",
       " 'google',\n",
       " 'google_drive',\n",
       " 'gov_uk',\n",
       " 'grupo_bancolombia',\n",
       " 'hfe',\n",
       " 'hsbc',\n",
       " 'htb',\n",
       " 'ics',\n",
       " 'ieee',\n",
       " 'impots_gov',\n",
       " 'infinisource',\n",
       " 'instagram',\n",
       " 'irs',\n",
       " 'itau',\n",
       " 'knab',\n",
       " 'la_banque_postale',\n",
       " 'la_poste',\n",
       " 'latam',\n",
       " 'lbb',\n",
       " 'lcl',\n",
       " 'linkedin',\n",
       " 'lloyds_bank',\n",
       " 'made_in_china',\n",
       " 'mbank',\n",
       " 'mdpd',\n",
       " 'mew',\n",
       " 'microsoft',\n",
       " 'momentum_office_design',\n",
       " 'ms_office',\n",
       " 'ms_onedrive',\n",
       " 'mweb',\n",
       " 'nab',\n",
       " 'natwest',\n",
       " 'navy_federal',\n",
       " 'nedbank',\n",
       " 'netflix',\n",
       " 'netsons',\n",
       " 'nordea',\n",
       " 'one_and_one',\n",
       " 'orange',\n",
       " 'orange_rockland',\n",
       " 'otrs',\n",
       " 'paypal',\n",
       " 'postbank',\n",
       " 'qnb',\n",
       " 'rbc',\n",
       " 'runescape',\n",
       " 'sharp',\n",
       " 'shoptet',\n",
       " 'sicil_shop',\n",
       " 'smartsheet',\n",
       " 'smiles',\n",
       " 'snapchat',\n",
       " 'sparkasse',\n",
       " 'standard_bank',\n",
       " 'steam',\n",
       " 'strato',\n",
       " 'stripe',\n",
       " 'summit_bank',\n",
       " 'sunrise',\n",
       " 'suntrust',\n",
       " 'swisscom',\n",
       " 'taxact',\n",
       " 'tech_target',\n",
       " 'telecom',\n",
       " 'test_rite',\n",
       " 'timeweb',\n",
       " 'tradekey',\n",
       " 'twins_bnk',\n",
       " 'twitter',\n",
       " 'typeform',\n",
       " 'usaa',\n",
       " 'walmart',\n",
       " 'wells_fargo',\n",
       " 'whatsapp',\n",
       " 'xtrix_tv',\n",
       " 'yahoo',\n",
       " 'youtube',\n",
       " 'ziggo',\n",
       " 'zoominfo']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "root = pathlib.Path('dataset/train')\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for p in vgg16.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in resnet50.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "classes_num = len(classes)\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=2048, out_features=512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(in_features=512, out_features=classes_num),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=2048, out_features=1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=1024, out_features=512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.6),\n",
    "    nn.Linear(in_features=512, out_features=classes_num),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "\n",
    "        out = self(images)  # Generate predictions\n",
    "\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "\n",
    "        out = self(images)  # Generate predictions\n",
    "\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WebVGGModel(ImageClassificationBase):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.network = model\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "\n",
    "class WebResNetModel(ImageClassificationBase):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.network = model\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_vgg = WebVGGModel(vgg16)\n",
    "model_resnet = WebResNetModel(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "pred_dl = DeviceDataLoader(pred_dl, device)\n",
    "to_device(model_vgg, device);\n",
    "to_device(model_resnet, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()  #eval() is called to tell model that now it is training mode and so  perform stuff like dropout,backpropagation etc..\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 4.975980758666992, 'val_acc': 0.012369791977107525}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg = to_device(model_vgg, device)\n",
    "evaluate(model_vgg, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 4.979844570159912, 'val_acc': 0.0026041667442768812}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet = to_device(model_resnet, device)\n",
    "evaluate(model_resnet, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 4.8486, val_loss: 4.9453, val_acc: 0.0280\n",
      "Epoch [1], train_loss: 4.5876, val_loss: 4.8884, val_acc: 0.0495\n",
      "Epoch [2], train_loss: 4.3422, val_loss: 4.7842, val_acc: 0.0794\n",
      "Epoch [3], train_loss: 4.1346, val_loss: 4.6893, val_acc: 0.0931\n",
      "Epoch [4], train_loss: 3.9587, val_loss: 4.6018, val_acc: 0.1016\n",
      "Epoch [5], train_loss: 3.8096, val_loss: 4.5254, val_acc: 0.1035\n",
      "Epoch [6], train_loss: 3.6592, val_loss: 4.4394, val_acc: 0.1237\n",
      "Epoch [7], train_loss: 3.5345, val_loss: 4.4134, val_acc: 0.1257\n",
      "Epoch [8], train_loss: 3.4378, val_loss: 4.3235, val_acc: 0.1406\n",
      "Epoch [9], train_loss: 3.3466, val_loss: 4.2348, val_acc: 0.1582\n",
      "Epoch [10], train_loss: 3.2533, val_loss: 4.1972, val_acc: 0.1628\n",
      "Epoch [11], train_loss: 3.1664, val_loss: 4.1589, val_acc: 0.1725\n",
      "Epoch [12], train_loss: 3.0969, val_loss: 4.0882, val_acc: 0.1869\n",
      "Epoch [13], train_loss: 3.0283, val_loss: 4.0491, val_acc: 0.1940\n",
      "Epoch [14], train_loss: 2.9451, val_loss: 3.9912, val_acc: 0.2005\n",
      "Epoch [15], train_loss: 2.8935, val_loss: 3.9538, val_acc: 0.2057\n",
      "Epoch [16], train_loss: 2.8231, val_loss: 3.9173, val_acc: 0.2162\n",
      "Epoch [17], train_loss: 2.7784, val_loss: 3.8742, val_acc: 0.2207\n",
      "Epoch [18], train_loss: 2.7100, val_loss: 3.8363, val_acc: 0.2331\n",
      "Epoch [19], train_loss: 2.6605, val_loss: 3.8148, val_acc: 0.2377\n",
      "Epoch [20], train_loss: 2.5980, val_loss: 3.7820, val_acc: 0.2383\n",
      "Epoch [21], train_loss: 2.5773, val_loss: 3.7443, val_acc: 0.2507\n",
      "Epoch [22], train_loss: 2.5267, val_loss: 3.7084, val_acc: 0.2520\n",
      "Epoch [23], train_loss: 2.4880, val_loss: 3.6653, val_acc: 0.2657\n",
      "Epoch [24], train_loss: 2.4357, val_loss: 3.6601, val_acc: 0.2689\n",
      "Epoch [25], train_loss: 2.3995, val_loss: 3.6128, val_acc: 0.2806\n",
      "Epoch [26], train_loss: 2.3542, val_loss: 3.5829, val_acc: 0.2865\n",
      "Epoch [27], train_loss: 2.3384, val_loss: 3.5471, val_acc: 0.2937\n",
      "Epoch [28], train_loss: 2.2809, val_loss: 3.5362, val_acc: 0.2989\n",
      "Epoch [29], train_loss: 2.2488, val_loss: 3.5044, val_acc: 0.3047\n",
      "Epoch [30], train_loss: 2.2136, val_loss: 3.4842, val_acc: 0.3100\n",
      "Epoch [31], train_loss: 2.1814, val_loss: 3.4412, val_acc: 0.3184\n",
      "Epoch [32], train_loss: 2.1430, val_loss: 3.4223, val_acc: 0.3275\n",
      "Epoch [33], train_loss: 2.1111, val_loss: 3.3826, val_acc: 0.3256\n",
      "Epoch [34], train_loss: 2.0832, val_loss: 3.3748, val_acc: 0.3353\n",
      "Epoch [35], train_loss: 2.0605, val_loss: 3.3430, val_acc: 0.3353\n",
      "Epoch [36], train_loss: 2.0097, val_loss: 3.3124, val_acc: 0.3432\n",
      "Epoch [37], train_loss: 2.0031, val_loss: 3.2969, val_acc: 0.3517\n",
      "Epoch [38], train_loss: 1.9554, val_loss: 3.2766, val_acc: 0.3503\n",
      "Epoch [39], train_loss: 1.9317, val_loss: 3.2547, val_acc: 0.3484\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.00001\n",
    "vgg_history = fit(num_epochs, lr, model_vgg, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 4.7981, val_loss: 5.0822, val_acc: 0.0273\n",
      "Epoch [1], train_loss: 4.5468, val_loss: 5.0009, val_acc: 0.0397\n",
      "Epoch [2], train_loss: 4.3064, val_loss: 4.9273, val_acc: 0.0586\n",
      "Epoch [3], train_loss: 4.0280, val_loss: 4.7698, val_acc: 0.0768\n",
      "Epoch [4], train_loss: 3.7858, val_loss: 4.6384, val_acc: 0.1055\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.0001\n",
    "resnet_history = fit(num_epochs, lr, model_resnet, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_single(input, label, model):\n",
    "    input = to_device(input, device)\n",
    "    inputs = input.unsqueeze(0)  # unsqueeze the input i.e. add an additonal dimension\n",
    "    predictions = model(inputs)\n",
    "    prediction = predictions[0].detach().cpu()\n",
    "    # print(f\"Prediction is {np.argmax(prediction)} of Model whereas given label is {label}\")\n",
    "    return np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62        10\n",
      "           1       0.50      0.70      0.58        10\n",
      "           2       0.06      0.10      0.08        10\n",
      "           3       0.50      1.00      0.67        10\n",
      "           4       0.47      0.70      0.56        10\n",
      "           5       0.62      0.50      0.56        10\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.50      0.10      0.17        10\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.25      0.20      0.22        10\n",
      "          10       0.89      0.80      0.84        10\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       0.70      0.70      0.70        10\n",
      "          13       0.46      0.60      0.52        10\n",
      "          14       1.00      0.30      0.46        10\n",
      "          15       0.11      0.60      0.19        10\n",
      "          16       1.00      0.60      0.75        10\n",
      "          17       1.00      0.80      0.89        10\n",
      "          18       0.00      0.00      0.00        10\n",
      "          19       0.00      0.00      0.00        10\n",
      "          20       0.47      0.90      0.62        10\n",
      "          21       1.00      0.10      0.18        10\n",
      "          22       0.11      0.20      0.14        10\n",
      "          23       0.14      0.20      0.17        10\n",
      "          24       0.00      0.00      0.00        10\n",
      "          25       1.00      0.80      0.89        10\n",
      "          26       0.53      0.80      0.64        10\n",
      "          27       0.09      0.90      0.16        10\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00        10\n",
      "          30       1.00      0.30      0.46        10\n",
      "          31       1.00      0.30      0.46        10\n",
      "          32       0.15      0.20      0.17        10\n",
      "          33       0.57      0.40      0.47        10\n",
      "          34       0.88      0.70      0.78        10\n",
      "          35       0.00      0.00      0.00        10\n",
      "          36       1.00      0.50      0.67        10\n",
      "          37       0.00      0.00      0.00        10\n",
      "          38       0.00      0.00      0.00        10\n",
      "          39       0.40      0.60      0.48        10\n",
      "          40       0.18      0.20      0.19        10\n",
      "          41       0.62      0.50      0.56        10\n",
      "          42       1.00      0.10      0.18        10\n",
      "          43       0.00      0.00      0.00        10\n",
      "          44       0.67      0.60      0.63        10\n",
      "          45       0.00      0.00      0.00        10\n",
      "          46       0.00      0.00      0.00        10\n",
      "          47       0.00      0.00      0.00        10\n",
      "          48       0.50      0.70      0.58        10\n",
      "          49       0.29      0.40      0.33        10\n",
      "          50       0.32      0.80      0.46        10\n",
      "          51       1.00      0.60      0.75        10\n",
      "          52       0.17      0.10      0.12        10\n",
      "          53       0.20      0.60      0.30        10\n",
      "          54       0.12      0.70      0.21        10\n",
      "          55       0.29      0.50      0.37        10\n",
      "          56       0.67      0.20      0.31        10\n",
      "          57       1.00      0.30      0.46        10\n",
      "          58       0.00      0.00      0.00        10\n",
      "          59       0.53      0.90      0.67        10\n",
      "          60       0.67      0.20      0.31        10\n",
      "          61       1.00      0.40      0.57        10\n",
      "          62       0.19      0.90      0.31        10\n",
      "          63       0.25      0.10      0.14        10\n",
      "          64       0.26      0.80      0.39        10\n",
      "          65       1.00      0.80      0.89        10\n",
      "          66       0.00      0.00      0.00        10\n",
      "          67       0.00      0.00      0.00        10\n",
      "          68       0.59      1.00      0.74        10\n",
      "          69       0.67      1.00      0.80        10\n",
      "          70       0.44      0.40      0.42        10\n",
      "          71       1.00      0.70      0.82        10\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       0.18      0.50      0.26        10\n",
      "          74       1.00      0.10      0.18        10\n",
      "          75       0.00      0.00      0.00        10\n",
      "          76       1.00      0.90      0.95        10\n",
      "          77       0.30      0.70      0.42        10\n",
      "          78       0.18      0.90      0.30        10\n",
      "          79       0.00      0.00      0.00        10\n",
      "          80       1.00      0.20      0.33        10\n",
      "          81       0.75      0.30      0.43        10\n",
      "          82       0.25      0.70      0.37        10\n",
      "          83       0.00      0.00      0.00        10\n",
      "          84       0.26      1.00      0.41        10\n",
      "          85       0.64      0.70      0.67        10\n",
      "          86       0.38      0.30      0.33        10\n",
      "          87       0.00      0.00      0.00        10\n",
      "          88       0.23      0.30      0.26        10\n",
      "          89       0.80      0.40      0.53        10\n",
      "          90       0.00      0.00      0.00        10\n",
      "          91       0.12      0.60      0.20        10\n",
      "          92       1.00      0.50      0.67        10\n",
      "          93       0.50      0.10      0.17        10\n",
      "          94       1.00      0.90      0.95        10\n",
      "          95       0.00      0.00      0.00        10\n",
      "          96       0.00      0.00      0.00        10\n",
      "          97       0.15      0.40      0.22        10\n",
      "          98       0.00      0.00      0.00        10\n",
      "          99       1.00      0.20      0.33        10\n",
      "         100       1.00      0.10      0.18        10\n",
      "         101       1.00      0.30      0.46        10\n",
      "         102       0.60      0.30      0.40        10\n",
      "         103       0.23      0.70      0.35        10\n",
      "         104       0.36      0.80      0.50        10\n",
      "         105       1.00      0.10      0.18        10\n",
      "         106       0.88      0.70      0.78        10\n",
      "         107       0.67      0.80      0.73        10\n",
      "         108       0.00      0.00      0.00        10\n",
      "         109       0.00      0.00      0.00        10\n",
      "         110       0.75      0.30      0.43        10\n",
      "         111       0.64      0.70      0.67        10\n",
      "         112       0.43      0.60      0.50        10\n",
      "         113       0.00      0.00      0.00        10\n",
      "         114       1.00      1.00      1.00        10\n",
      "         115       0.80      0.80      0.80        10\n",
      "         116       0.37      0.70      0.48        10\n",
      "         117       0.25      0.40      0.31        10\n",
      "         118       0.71      0.50      0.59        10\n",
      "         119       0.50      0.80      0.62        10\n",
      "         120       0.56      0.90      0.69        10\n",
      "         121       0.00      0.00      0.00        10\n",
      "         122       0.00      0.00      0.00        10\n",
      "         123       0.39      0.90      0.55        10\n",
      "         124       0.00      0.00      0.00        10\n",
      "         125       0.33      0.20      0.25        10\n",
      "         126       0.00      0.00      0.00        10\n",
      "         127       0.00      0.00      0.00        10\n",
      "         128       0.40      0.20      0.27        10\n",
      "         129       0.00      0.00      0.00        10\n",
      "         130       0.00      0.00      0.00        10\n",
      "         131       0.00      0.00      0.00        10\n",
      "         132       0.00      0.00      0.00        10\n",
      "         133       0.00      0.00      0.00        10\n",
      "         134       0.43      1.00      0.61        10\n",
      "         135       0.25      0.20      0.22        10\n",
      "         136       0.83      0.50      0.62        10\n",
      "         137       0.50      1.00      0.67        10\n",
      "         138       1.00      0.20      0.33        10\n",
      "         139       1.00      0.80      0.89        10\n",
      "         140       1.00      0.10      0.18        10\n",
      "         141       0.50      0.60      0.55        10\n",
      "         142       0.00      0.00      0.00        10\n",
      "         143       0.00      0.00      0.00        10\n",
      "         144       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.38      1450\n",
      "   macro avg       0.41      0.38      0.33      1450\n",
      "weighted avg       0.41      0.38      0.33      1450\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "分类报告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.80      0.40      0.53        10\n",
      "           4       0.09      0.90      0.17        10\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00        10\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       0.21      0.60      0.32        10\n",
      "          13       0.00      0.00      0.00        10\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       0.04      0.20      0.07        10\n",
      "          16       0.00      0.00      0.00        10\n",
      "          17       0.00      0.00      0.00        10\n",
      "          18       0.00      0.00      0.00        10\n",
      "          19       0.00      0.00      0.00        10\n",
      "          20       0.00      0.00      0.00        10\n",
      "          21       0.00      0.00      0.00        10\n",
      "          22       0.00      0.00      0.00        10\n",
      "          23       0.00      0.00      0.00        10\n",
      "          24       0.00      0.00      0.00        10\n",
      "          25       0.00      0.00      0.00        10\n",
      "          26       0.15      0.60      0.24        10\n",
      "          27       0.04      0.90      0.07        10\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00        10\n",
      "          30       0.00      0.00      0.00        10\n",
      "          31       0.00      0.00      0.00        10\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00        10\n",
      "          34       0.00      0.00      0.00        10\n",
      "          35       0.00      0.00      0.00        10\n",
      "          36       0.00      0.00      0.00        10\n",
      "          37       0.00      0.00      0.00        10\n",
      "          38       0.00      0.00      0.00        10\n",
      "          39       0.14      0.10      0.12        10\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00        10\n",
      "          42       0.00      0.00      0.00        10\n",
      "          43       0.00      0.00      0.00        10\n",
      "          44       0.00      0.00      0.00        10\n",
      "          45       0.00      0.00      0.00        10\n",
      "          46       0.00      0.00      0.00        10\n",
      "          47       0.00      0.00      0.00        10\n",
      "          48       0.00      0.00      0.00        10\n",
      "          49       0.00      0.00      0.00        10\n",
      "          50       0.62      0.50      0.56        10\n",
      "          51       0.00      0.00      0.00        10\n",
      "          52       0.00      0.00      0.00        10\n",
      "          53       0.00      0.00      0.00        10\n",
      "          54       0.10      0.70      0.18        10\n",
      "          55       0.00      0.00      0.00        10\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00        10\n",
      "          58       0.00      0.00      0.00        10\n",
      "          59       1.00      0.50      0.67        10\n",
      "          60       0.00      0.00      0.00        10\n",
      "          61       0.00      0.00      0.00        10\n",
      "          62       0.10      0.90      0.17        10\n",
      "          63       0.00      0.00      0.00        10\n",
      "          64       0.05      1.00      0.10        10\n",
      "          65       0.00      0.00      0.00        10\n",
      "          66       0.00      0.00      0.00        10\n",
      "          67       0.00      0.00      0.00        10\n",
      "          68       0.07      0.70      0.12        10\n",
      "          69       0.00      0.00      0.00        10\n",
      "          70       0.00      0.00      0.00        10\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       1.00      0.10      0.18        10\n",
      "          74       0.00      0.00      0.00        10\n",
      "          75       0.00      0.00      0.00        10\n",
      "          76       1.00      0.60      0.75        10\n",
      "          77       0.00      0.00      0.00        10\n",
      "          78       0.24      0.90      0.38        10\n",
      "          79       0.00      0.00      0.00        10\n",
      "          80       0.00      0.00      0.00        10\n",
      "          81       0.00      0.00      0.00        10\n",
      "          82       0.09      0.60      0.16        10\n",
      "          83       0.00      0.00      0.00        10\n",
      "          84       0.12      1.00      0.22        10\n",
      "          85       0.00      0.00      0.00        10\n",
      "          86       0.00      0.00      0.00        10\n",
      "          87       0.00      0.00      0.00        10\n",
      "          88       0.00      0.00      0.00        10\n",
      "          89       0.00      0.00      0.00        10\n",
      "          90       0.00      0.00      0.00        10\n",
      "          91       0.03      0.20      0.05        10\n",
      "          92       0.00      0.00      0.00        10\n",
      "          93       0.00      0.00      0.00        10\n",
      "          94       0.00      0.00      0.00        10\n",
      "          95       0.00      0.00      0.00        10\n",
      "          96       0.00      0.00      0.00        10\n",
      "          97       0.00      0.00      0.00        10\n",
      "          98       0.00      0.00      0.00        10\n",
      "          99       0.00      0.00      0.00        10\n",
      "         100       0.00      0.00      0.00        10\n",
      "         101       0.00      0.00      0.00        10\n",
      "         102       0.00      0.00      0.00        10\n",
      "         103       0.00      0.00      0.00        10\n",
      "         104       0.00      0.00      0.00        10\n",
      "         105       0.00      0.00      0.00        10\n",
      "         106       0.00      0.00      0.00        10\n",
      "         107       0.50      0.40      0.44        10\n",
      "         108       0.00      0.00      0.00        10\n",
      "         109       0.00      0.00      0.00        10\n",
      "         110       0.00      0.00      0.00        10\n",
      "         111       0.00      0.00      0.00        10\n",
      "         112       0.00      0.00      0.00        10\n",
      "         113       0.00      0.00      0.00        10\n",
      "         114       1.00      0.80      0.89        10\n",
      "         115       0.00      0.00      0.00        10\n",
      "         116       0.00      0.00      0.00        10\n",
      "         117       0.00      0.00      0.00        10\n",
      "         118       1.00      0.50      0.67        10\n",
      "         119       0.13      0.50      0.21        10\n",
      "         120       0.22      1.00      0.36        10\n",
      "         121       0.00      0.00      0.00        10\n",
      "         122       0.00      0.00      0.00        10\n",
      "         123       0.13      0.90      0.23        10\n",
      "         124       0.00      0.00      0.00        10\n",
      "         125       0.00      0.00      0.00        10\n",
      "         126       0.00      0.00      0.00        10\n",
      "         127       0.00      0.00      0.00        10\n",
      "         128       0.00      0.00      0.00        10\n",
      "         129       0.00      0.00      0.00        10\n",
      "         130       0.00      0.00      0.00        10\n",
      "         131       0.00      0.00      0.00        10\n",
      "         132       0.00      0.00      0.00        10\n",
      "         133       0.00      0.00      0.00        10\n",
      "         134       0.19      0.60      0.29        10\n",
      "         135       0.00      0.00      0.00        10\n",
      "         136       0.00      0.00      0.00        10\n",
      "         137       0.17      0.40      0.24        10\n",
      "         138       0.00      0.00      0.00        10\n",
      "         139       0.57      0.80      0.67        10\n",
      "         140       0.00      0.00      0.00        10\n",
      "         141       0.00      0.00      0.00        10\n",
      "         142       0.00      0.00      0.00        10\n",
      "         143       0.00      0.00      0.00        10\n",
      "         144       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.12      1450\n",
      "   macro avg       0.07      0.12      0.06      1450\n",
      "weighted avg       0.07      0.12      0.06      1450\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models = [model_vgg, model_resnet]\n",
    "for modell in models:\n",
    "    labels = []\n",
    "    predicts = []\n",
    "    for i, img in enumerate(pred_ds):\n",
    "        result = predict_single(img[0], img[1], modell)\n",
    "        labels.append(img[1])\n",
    "        predicts.append(result)\n",
    "    print(\"分类报告\")\n",
    "    report = classification_report(labels,predicts,zero_division=0)\n",
    "    print(report)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model_vgg.state_dict(), 'web_model_vgg.pth')\n",
    "torch.save(model_resnet.state_dict(), 'web_model_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/mi/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "attention_net(\n",
       "  (pretrained_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (fc): Linear(in_features=2048, out_features=200, bias=True)\n",
       "  )\n",
       "  (proposal_net): ProposalNet(\n",
       "    (down1): Conv2d(2048, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (down3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (ReLU): ReLU()\n",
       "    (tidy1): Conv2d(128, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tidy2): Conv2d(128, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tidy3): Conv2d(128, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (concat_net): Linear(in_features=10240, out_features=200, bias=True)\n",
       "  (partcls_net): Linear(in_features=2048, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core import model\n",
    "\n",
    "net = model.attention_net(topN=3)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"ABC abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "057cd6e91eb50813ca0e8f270b2a1a3c0ac0da70aa11f11ef803f5bc61fb1dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('py36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
